{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99efef4e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13996/1610394484.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m         \u001b[0moutliers_mahal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrobust_mahalanobis_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m         \u001b[0moutliers_mahal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutliers_mahal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[0moutliers_mahal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutliers_mahal\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moutliers_mahal\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m5000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13996/1610394484.py\u001b[0m in \u001b[0;36mrobust_mahalanobis_method\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mx_minus_mu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mrobust_mean\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mleft_term\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_minus_mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minv_covmat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0mmahal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleft_term\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_minus_mu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m     \u001b[0mmd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmahal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiagonal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import make_regression\n",
    "import scipy as sp\n",
    "from scipy.stats import chi2\n",
    "from sklearn.covariance import MinCovDet\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Robust Mahalonibis Distance\n",
    "def robust_mahalanobis_method(df):\n",
    "    # Minimum covariance determinant\n",
    "    rng = np.random.RandomState(0)\n",
    "    real_cov = np.cov(df.values.T)\n",
    "    X = rng.multivariate_normal(mean=np.mean(df, axis=0), cov=real_cov, size=506)\n",
    "    cov = MinCovDet(random_state=42).fit(X)\n",
    "    mcd = cov.covariance_  # robust covariance metric\n",
    "    robust_mean = cov.location_  # robust mean\n",
    "    inv_covmat = sp.linalg.inv(mcd)  # inverse covariance metric\n",
    "\n",
    "    # Robust M-Distance\n",
    "    x_minus_mu = df - robust_mean\n",
    "    left_term = np.dot(x_minus_mu, inv_covmat)\n",
    "    mahal = np.dot(left_term, x_minus_mu.T)\n",
    "    md = np.sqrt(mahal.diagonal())\n",
    "\n",
    "    # Flag as outlier\n",
    "    outlier = []\n",
    "    C = np.sqrt(\n",
    "        chi2.ppf((1 - 0.001), df=df.shape[1])\n",
    "    )  # degrees of freedom = number of variables\n",
    "    for index, value in enumerate(md):\n",
    "        if value > C:\n",
    "            outlier.append(index)\n",
    "        else:\n",
    "            continue\n",
    "    return outlier, md\n",
    "\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "performance = []\n",
    "\n",
    "counter = 0\n",
    "for std in range(0, 100, 10):\n",
    "    performance.append([std, []])\n",
    "    for outlier_num in range(0, 5000, 100):\n",
    "        # Generate regression dataset\n",
    "        X, y = make_regression(\n",
    "            n_samples=5000,\n",
    "            n_features=1,\n",
    "            noise=0.0,\n",
    "            bias=0.0,\n",
    "            random_state=42,\n",
    "        )\n",
    "\n",
    "        # Generate regression dataset\n",
    "        X_noisy, y_noisy = make_regression(\n",
    "            n_samples=5000,\n",
    "            n_features=1,\n",
    "            noise=std,\n",
    "            bias=0.0,\n",
    "            random_state=42,\n",
    "        )\n",
    "\n",
    "        X_outliers = []\n",
    "        y_outliers = []\n",
    "        for i in range(outlier_num):\n",
    "            X_outliers = np.append(X_outliers, rng.choice(X_noisy.flatten()))\n",
    "            y_outliers = np.append(y_outliers, rng.choice(y_noisy.flatten()))\n",
    "\n",
    "        data = np.stack((np.append(X, X_outliers), np.append(y, y_outliers)), axis=1)\n",
    "\n",
    "        df = pd.DataFrame(data)\n",
    "        outliers_mahal, md = robust_mahalanobis_method(df=df)\n",
    "        outliers_mahal = np.array(outliers_mahal)\n",
    "        outliers_mahal = outliers_mahal[outliers_mahal > 5000]\n",
    "\n",
    "        X_outliers_final = []\n",
    "        y_outliers_final = []\n",
    "\n",
    "        X_temp = np.append(X, X_outliers)\n",
    "        y_temp = np.append(y, y_outliers)\n",
    "        for i in outliers_mahal:\n",
    "            X_outliers_final = np.append(X_outliers_final, X_temp[i])\n",
    "            y_outliers_final = np.append(y_outliers_final, y_temp[i])\n",
    "\n",
    "        X = np.append(X, X_outliers_final)\n",
    "        y = np.append(y, y_outliers_final)\n",
    "\n",
    "        X = X.reshape(-1, 1)\n",
    "        y = y.reshape(-1, 1)\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42\n",
    "        )\n",
    "\n",
    "        regressor = LinearRegression()\n",
    "        regressor.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = regressor.predict(X_test)\n",
    "\n",
    "        performance[counter][1].append(\n",
    "            [\n",
    "                outlier_num,\n",
    "                metrics.r2_score(y_test, y_pred),\n",
    "                metrics.mean_absolute_error(y_test, y_pred),\n",
    "                metrics.mean_squared_error(y_test, y_pred),\n",
    "                np.sqrt(metrics.mean_squared_error(y_test, y_pred)),\n",
    "                regressor.intercept_,\n",
    "                regressor.coef_,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # print(\"R2 Score:\", metrics.r2_score(y_test, y_pred))\n",
    "        # print(\"Mean Absolute Error:\", metrics.mean_absolute_error(y_test, y_pred))\n",
    "        # print(\"Mean Squared Error:\", metrics.mean_squared_error(y_test, y_pred))\n",
    "        # print(\n",
    "        #     \"Root Mean Squared Error:\",\n",
    "        #     np.sqrt(metrics.mean_squared_error(y_test, y_pred)),\n",
    "        # )\n",
    "    print(\"Iteration: \" + str(counter) + \"...\")\n",
    "    counter += 1\n",
    "\n",
    "# df = pd.DataFrame({\"Actual\": y_test.flatten(), \"Predicted\": y_pred.flatten()})\n",
    "# df.head(25).plot(kind=\"bar\", figsize=(16, 10))\n",
    "# plt.grid(which=\"major\", linestyle=\"-\", linewidth=\"0.5\", color=\"green\")\n",
    "# plt.grid(which=\"minor\", linestyle=\":\", linewidth=\"0.5\", color=\"black\")\n",
    "# plt.show()\n",
    "\n",
    "fig, axs = plt.subplots(3, 2)\n",
    "performance[0][0] = 1\n",
    "for std in performance:\n",
    "    stats = [[], [], [], [], [], [], []]\n",
    "    for i in std[1]:\n",
    "        stats[0].append(i[0])\n",
    "        stats[1].append(i[1])\n",
    "        stats[2].append(i[2])\n",
    "        stats[3].append(i[3])\n",
    "        stats[4].append(i[4])\n",
    "\n",
    "    axs[0, 0].plot(stats[0], stats[1], linewidth=2, label=\"Std = {:.0f}\".format(std[0]))\n",
    "    axs[0, 0].set_xlabel(\"Number of Outliers\")\n",
    "    axs[0, 0].set_ylabel(\"R2 Score\")\n",
    "    axs[0, 0].legend(loc=\"lower right\")\n",
    "\n",
    "    axs[0, 1].plot(stats[0], stats[2], linewidth=2, label=\"Std = {:.0f}\".format(std[0]))\n",
    "    axs[0, 1].set_xlabel(\"Number of Outliers\")\n",
    "    axs[0, 1].set_ylabel(\"MAE\")\n",
    "    axs[0, 1].legend(loc=\"lower right\")\n",
    "\n",
    "    axs[1, 0].plot(stats[0], stats[3], linewidth=2, label=\"Std = {:.0f}\".format(std[0]))\n",
    "    axs[1, 0].set_xlabel(\"Number of Outliers\")\n",
    "    axs[1, 0].set_ylabel(\"MSE\")\n",
    "    axs[1, 0].legend(loc=\"lower right\")\n",
    "\n",
    "    axs[1, 1].plot(stats[0], stats[4], linewidth=2, label=\"Std = {:.0f}\".format(std[0]))\n",
    "    axs[1, 1].set_xlabel(\"Number of Outliers\")\n",
    "    axs[1, 1].set_ylabel(\"RMSE\")\n",
    "    axs[1, 1].legend(loc=\"lower right\")\n",
    "\n",
    "    axs[2, 0].plot(stats[0], stats[4], linewidth=2, label=\"Std = {:.0f}\".format(std[0]))\n",
    "    axs[2, 0].set_xlabel(\"Number of Outliers\")\n",
    "    axs[2, 0].set_ylabel(\"Intercept\")\n",
    "    axs[2, 0].legend(loc=\"lower right\")\n",
    "\n",
    "    axs[2, 1].plot(stats[0], stats[4], linewidth=2, label=\"Std = {:.0f}\".format(std[0]))\n",
    "    axs[2, 1].set_xlabel(\"Number of Outliers\")\n",
    "    axs[2, 1].set_ylabel(\"Estimated Coefficient\")\n",
    "    axs[2, 1].legend(loc=\"lower right\")\n",
    "\n",
    "# # Generate regression dataset\n",
    "# X, y = make_regression(\n",
    "#     n_samples=5000,\n",
    "#     n_features=1,\n",
    "#     noise=0.0,\n",
    "#     bias=0.0,\n",
    "#     random_state=42,\n",
    "# )\n",
    "\n",
    "# X_noisy, y_noisy = make_regression(\n",
    "#     n_samples=5000,\n",
    "#     n_features=1,\n",
    "#     noise=0.0,\n",
    "#     bias=0.0,\n",
    "#     random_state=42,\n",
    "# )\n",
    "\n",
    "# # X_outliers = []\n",
    "# # y_outliers = []\n",
    "# # for i in range(1000):\n",
    "# #     X_outliers = np.append(X_outliers, rng.choice(X_noisy.flatten()))\n",
    "# #     y_outliers = np.append(y_outliers, rng.choice(y_noisy.flatten()))\n",
    "\n",
    "# data = np.stack((np.append(X, X_noisy), np.append(y, y_noisy)), axis=1)\n",
    "\n",
    "# df = pd.DataFrame(data)\n",
    "# outliers_mahal, md = robust_mahalanobis_method(df=df)\n",
    "# outliers_mahal = np.array(outliers_mahal)\n",
    "# outliers_mahal = outliers_mahal[outliers_mahal > 5000]\n",
    "\n",
    "# X_outliers_final = []\n",
    "# y_outliers_final = []\n",
    "\n",
    "# X_temp = np.append(X, X_noisy)\n",
    "# y_temp = np.append(y, y_noisy)\n",
    "# for i in outliers_mahal:\n",
    "#     X_outliers_final = np.append(X_outliers_final, X_temp[i])\n",
    "#     y_outliers_final = np.append(y_outliers_final, y_temp[i])\n",
    "\n",
    "# X = np.append(X, X_outliers_final)\n",
    "# y = np.append(y, y_outliers_final)\n",
    "\n",
    "# X = X.reshape(-1, 1)\n",
    "# y = y.reshape(-1, 1)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     X, y, test_size=0.2, random_state=42\n",
    "# )\n",
    "\n",
    "# regressor = LinearRegression()\n",
    "# regressor.fit(X_train, y_train)\n",
    "\n",
    "# y_pred = regressor.predict(X_test)\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.scatter(X, y, label=\"Original Data\")\n",
    "# ax.scatter(X_outliers_final, y_outliers_final, c=\"Purple\", label=\"Outliers\")\n",
    "# ax.plot(X_test, y_pred, color=\"Red\", linewidth=2, label=\"Intercept\")\n",
    "# ax.annotate(\n",
    "#     \"R2 Score = {:.3f}\".format(metrics.r2_score(y_test, y_pred)),\n",
    "#     xy=(0.015, 0.98),\n",
    "#     xycoords=\"axes fraction\",\n",
    "#     verticalalignment=\"top\",\n",
    "#     horizontalalignment=\"left\",\n",
    "# )\n",
    "# ax.annotate(\n",
    "#     \"Mean Absolute Error = {:.3f}\\n\".format(\n",
    "#         metrics.mean_absolute_error(y_test, y_pred)\n",
    "#     ),\n",
    "#     xy=(0.015, 0.935),\n",
    "#     xycoords=\"axes fraction\",\n",
    "#     verticalalignment=\"top\",\n",
    "#     horizontalalignment=\"left\",\n",
    "# )\n",
    "# ax.annotate(\n",
    "#     \"Mean Squared Error = {:.3f}\\n\".format(metrics.mean_squared_error(y_test, y_pred)),\n",
    "#     xy=(0.015, 0.886),\n",
    "#     xycoords=\"axes fraction\",\n",
    "#     verticalalignment=\"top\",\n",
    "#     horizontalalignment=\"left\",\n",
    "# )\n",
    "# ax.annotate(\n",
    "#     \"Root Mean Squared Error = {:.3f}\".format(\n",
    "#         np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
    "#     ),\n",
    "#     xy=(0.015, 0.84),\n",
    "#     xycoords=\"axes fraction\",\n",
    "#     verticalalignment=\"top\",\n",
    "#     horizontalalignment=\"left\",\n",
    "# )\n",
    "# ax.annotate(\n",
    "#     \"Intercept = {:.3f}\".format(regressor.intercept_[0]),\n",
    "#     xy=(0.015, 0.795),\n",
    "#     xycoords=\"axes fraction\",\n",
    "#     verticalalignment=\"top\",\n",
    "#     horizontalalignment=\"left\",\n",
    "# )\n",
    "# ax.annotate(\n",
    "#     \"Estimated Coefficient = {:.3f}\".format(regressor.coef_[0][0]),\n",
    "#     xy=(0.015, 0.75),\n",
    "#     xycoords=\"axes fraction\",\n",
    "#     verticalalignment=\"top\",\n",
    "#     horizontalalignment=\"left\",\n",
    "# )\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04a7263",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
